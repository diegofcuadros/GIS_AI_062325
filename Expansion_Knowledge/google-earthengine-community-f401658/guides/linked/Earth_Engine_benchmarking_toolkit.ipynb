{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "IWAZOngtsPza"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpubVj8sL0Ns"
      },
      "outputs": [],
      "source": [
        "#@title Copyright 2024 Google LLC. { display-mode: \"form\" }\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Earth Engine benchmarking toolkit\n",
        "\n",
        "This notebook provides a set of tools for benchmarking the EECU-time cost of\n",
        "a number of different types of Earth Engine processing operations.\n",
        "\n",
        "For more information about how to use this notebook to estimate Earth Enigne costs, please see [the developers' guide](https://developers.google.com/earth-engine/guides/computation_benchmarks)"
      ],
      "metadata": {
        "id": "zXuJtS66rJxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import cache\n",
        "from google.colab import auth\n",
        "from google.api_core import retry\n",
        "from numpy.lib import recfunctions as rfn\n",
        "from tqdm import tqdm\n",
        "from tqdm.notebook import tqdm_notebook\n",
        "\n",
        "import concurrent\n",
        "import ee\n",
        "import io\n",
        "import google\n",
        "import logging\n",
        "import numpy as np\n",
        "import requests\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "paJb6oU1L2w3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = \"your-project-here\" # @param {type:\"string\"}\n",
        "BUCKET_NAME = \"your-bucket-here\" # @param {type:\"string\"}\n",
        "RANDOM_SEED = 1 # @param {type:\"integer\"}\n",
        "HV_DRY_RUN = True # @param {type:\"boolean\"}\n",
        "TASK_DRY_RUN = True # @param {type:\"boolean\"}"
      ],
      "metadata": {
        "id": "muK6LYbYL5qV",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Grab the right creds.\n",
        "auth.authenticate_user()\n",
        "credentials, _ = google.auth.default()\n",
        "\n",
        "# Initialize Earth Engine.\n",
        "ee.Initialize(\n",
        "    credentials,\n",
        "    project=PROJECT_ID,\n",
        "    opt_url='https://earthengine-highvolume.googleapis.com')"
      ],
      "metadata": {
        "id": "ZPwQhMvDMBmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# High-volume data extraction\n",
        "\n",
        "Copied from the [\"Pixels to the People!\" blog post from Nick Clinton](https://medium.com/google-earth/pixels-to-the-people-2d3c14a46da6). This issues a large number of queries to the Earth Engine online stack."
      ],
      "metadata": {
        "id": "IWAZOngtsPza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@cache\n",
        "def get_proj(scale):\n",
        "  \"\"\"Return the EPSG:4326 projection at the given nominal scale.\"\"\"\n",
        "  return ee.Projection('EPSG:4326').atScale(scale).getInfo()\n",
        "\n",
        "@retry.Retry()\n",
        "def get_patch(coords, image, scale, patch_size=128):\n",
        "  \"\"\"Get a patch centered on the given point, as a numpy array.\"\"\"\n",
        "  # Pre-compute a geographic coordinate system.\n",
        "  proj = get_proj(scale)\n",
        "\n",
        "  # Get scales in degrees out of the transform.\n",
        "  scale_x = proj['transform'][0]\n",
        "  scale_y = -proj['transform'][4]\n",
        "\n",
        "  # Offset to the upper left corner.\n",
        "  offset_x = -scale_x * patch_size / 2\n",
        "  offset_y = -scale_y * patch_size / 2\n",
        "\n",
        "  request = {\n",
        "      'fileFormat': 'NPY',\n",
        "      'grid': {\n",
        "          'dimensions': {\n",
        "              'width': patch_size,\n",
        "              'height': patch_size\n",
        "          },\n",
        "          'affineTransform': {\n",
        "              'scaleX': scale_x,\n",
        "              'shearX': 0,\n",
        "              'shearY': 0,\n",
        "              'scaleY': scale_y,\n",
        "              'translateX': coords[0] + offset_x,\n",
        "              'translateY': coords[1] + offset_y,\n",
        "          },\n",
        "          'crsCode': proj['crs']\n",
        "      },\n",
        "      'expression': image,\n",
        "  }\n",
        "  return np.load(io.BytesIO(ee.data.computePixels(request)))\n",
        "\n",
        "\n",
        "@cache\n",
        "def get_sample_coords(roi, n):\n",
        "  \"\"\"\"Get a random sample of N points in the ROI.\"\"\"\n",
        "  points = ee.FeatureCollection.randomPoints(region=roi, points=n, maxError=1, seed=RANDOM_SEED)\n",
        "  return points.aggregate_array('.geo').getInfo()\n",
        "\n",
        "\n",
        "def array_to_example(structured_array, features):\n",
        "  \"\"\"\"Serialize a structured numpy array into a tf.Example proto.\"\"\"\n",
        "  feature = {}\n",
        "  for f in features:\n",
        "    feature[f] = tf.train.Feature(\n",
        "        float_list = tf.train.FloatList(\n",
        "            value = structured_array[f].flatten()))\n",
        "  return tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "\n",
        "def extract_samples(image, roi, num_samples, scale, features, filename):\n",
        "  if HV_DRY_RUN:\n",
        "    print(f\"[Extracting {filename}]\")\n",
        "    return\n",
        "  sample_points = get_sample_coords(roi, num_samples)\n",
        "  writer = tf.io.TFRecordWriter(f'gs://{BUCKET_NAME}/{filename}.tfrecord.gz')\n",
        "\n",
        "  def write(writer, result):\n",
        "      try:\n",
        "          example_proto = array_to_example(result, features)\n",
        "          writer.write(example_proto.SerializeToString())\n",
        "      except Exception as e:\n",
        "          print(e)\n",
        "          pass\n",
        "\n",
        "  EXECUTOR = concurrent.futures.ThreadPoolExecutor(max_workers=HTTP_PARALLELISM)\n",
        "  progress_monitor = tqdm_notebook(\n",
        "      EXECUTOR.map(\n",
        "          lambda x: write(writer, get_patch(x['coordinates'], image, scale)),\n",
        "          sample_points),\n",
        "      desc=filename,\n",
        "      total=num_samples)\n",
        "\n",
        "  # Remove spurious warnings about 429s (since we retry them anyway).\n",
        "  class No429Filter(logging.Filter):\n",
        "    def filter(self, record):\n",
        "        # Don't allow 429 messages with log level WARNING.\n",
        "        return not (\"429\" in record.getMessage() and record.levelno == logging.WARNING)\n",
        "\n",
        "  filter = No429Filter()\n",
        "  try:\n",
        "    logging.getLogger(\"googleapiclient.http\").addFilter(filter)\n",
        "    result = list(progress_monitor)\n",
        "  finally:\n",
        "    logging.getLogger(\"googleapiclient.http\").removeFilter(filter)\n",
        "    writer.flush()\n",
        "    writer.close()"
      ],
      "metadata": {
        "id": "k8vrFVK1P9fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentinel-2 Composite\n",
        "\n",
        "A simple mosaic based on cloud-filtered images."
      ],
      "metadata": {
        "id": "YsgHGvd7SyTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Blue, green, red, NIR, AOT.\n",
        "S2_FEATURES = ['B2_median', 'B3_median', 'B4_median', 'B8_median', 'AOT_median']\n",
        "\n",
        "def get_s2_median(start, end):\n",
        "  \"\"\"Get a Sentinel-2 median composite in the ROI.\"\"\"\n",
        "  s2 = ee.ImageCollection('COPERNICUS/S2_HARMONIZED')\n",
        "  s2c = ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY')\n",
        "  s2Sr = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n",
        "\n",
        "  s2c = s2c.filterDate(start, end)\n",
        "  s2Sr = s2Sr.filterDate(start, end)\n",
        "\n",
        "  def indexJoin(collectionA, collectionB, propertyName):\n",
        "    joined = ee.ImageCollection(ee.Join.saveFirst(propertyName).apply(\n",
        "        primary=collectionA,\n",
        "        secondary=collectionB,\n",
        "        condition=ee.Filter.equals(\n",
        "            leftField='system:index',\n",
        "            rightField='system:index'\n",
        "        ))\n",
        "    )\n",
        "    return joined.map(lambda image : image.addBands(ee.Image(image.get(propertyName))))\n",
        "\n",
        "  def maskImage(image):\n",
        "    s2c = image.select('probability')\n",
        "    return image.updateMask(s2c.lt(50))\n",
        "\n",
        "  withCloudProbability = indexJoin(s2Sr, s2c, 'cloud_probability')\n",
        "  masked = ee.ImageCollection(withCloudProbability.map(maskImage))\n",
        "  return masked.reduce(ee.Reducer.median(), 8)"
      ],
      "metadata": {
        "id": "Ysta-2tgqYXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the benchmarks\n",
        "\n",
        "Caution! This can be expensive."
      ],
      "metadata": {
        "id": "enCP7LaATBfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Regions of interest.\n",
        "regions = {\n",
        "    'bay_area': ee.Geometry.Rectangle(\n",
        "       [-123.05832753906247, 37.03109527141115, -121.14121328124997, 38.24468432993584]),\n",
        "#   'nigeria': ee.FeatureCollection(\"USDOS/LSIB_SIMPLE/2017\")\\\n",
        "#       .filter(ee.Filter.eq('country_na', 'Nigeria'))\\\n",
        "#       .first()\\\n",
        "#       .geometry(),\n",
        "#   'germany': ee.FeatureCollection(\"USDOS/LSIB_SIMPLE/2017\")\\\n",
        "#     .filter(ee.Filter.eq('country_na', 'Germany'))\\\n",
        "#     .first()\\\n",
        "#     .geometry(),\n",
        "}\n",
        "\n",
        "BASE_DATE = ee.Date('2024-01-01')\n",
        "timeframes = {\n",
        "    '3mo': ee.DateRange(BASE_DATE.advance(-3, 'month'), BASE_DATE),\n",
        "#    '6mo': ee.DateRange(BASE_DATE.advance(-6, 'month'), BASE_DATE),\n",
        "#    '1yr': ee.DateRange(BASE_DATE.advance(-1, 'year'), BASE_DATE),\n",
        "}\n",
        "\n",
        "scales = [120] # [10, 30, 120]\n",
        "sample_count = [100] # [100, 500, 1000]\n",
        "\n",
        "for region_name in regions:\n",
        "  for timeframe in timeframes:\n",
        "    image = get_s2_median(timeframes[timeframe].start(), timeframes[timeframe].end())\n",
        "    for scale in scales:\n",
        "      operation_name = f'{region_name}_{timeframe}_{scale}m_image'\n",
        "\n",
        "      # Kick off an export job for the image.\n",
        "      image_op = f'{operation_name}_0samples_exportimage'\n",
        "      ee.data.setWorkloadTag(image_op)\n",
        "      image_task = ee.batch.Export.image.toCloudStorage(\n",
        "          image=image,\n",
        "          bucket=BUCKET_NAME,\n",
        "          fileNamePrefix=image_op,\n",
        "          region=regions[region_name],\n",
        "          scale=scale,\n",
        "          description=image_op,\n",
        "          maxPixels=2e10\n",
        "      )\n",
        "\n",
        "      if not TASK_DRY_RUN:\n",
        "        image_task.start()\n",
        "\n",
        "      for n in sample_count:\n",
        "        # Export point samples to BQ.\n",
        "        bq_op = f\"{operation_name}_{n}samples_bq\"\n",
        "        ee.data.setWorkloadTag(bq_op)\n",
        "        table = image.sample(\n",
        "                region=regions[region_name],\n",
        "                scale=scale,\n",
        "                numPixels=n,\n",
        "                tileScale=4,\n",
        "                seed=RANDOM_SEED)\n",
        "        bq_task = ee.batch.Export.table.toBigQuery(\n",
        "            collection=table,\n",
        "            description=bq_op,\n",
        "            table=f\"{PROJECT_ID}.{BUCKET_NAME}.{bq_op}\",\n",
        "        )\n",
        "        if not TASK_DRY_RUN:\n",
        "          bq_task.start()\n",
        "        else:\n",
        "          print(f\"[dry run] Starting {bq_task}\")\n",
        "\n",
        "        # Extract sample patches using the EE HV API.\n",
        "        hv_op = f\"{operation_name}_{n}samples_hv\"\n",
        "        ee.data.setWorkloadTag(hv_op)\n",
        "        extract_samples(image=image,\n",
        "                        roi=regions[region_name],\n",
        "                        num_samples=n,\n",
        "                        scale=scale,\n",
        "                        features=S2_FEATURES,\n",
        "                        filename=hv_op)"
      ],
      "metadata": {
        "id": "371lCEApMhWv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}